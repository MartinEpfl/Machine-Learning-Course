{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import datetime\n",
    "from costs import compute_loss\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = \"train.csv\"\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "# standardize the data\n",
    "tX_std = standardize(tX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement ML methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## least squares GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    # ***************************************************\n",
    "    return (-1/len(y))*tx.T@(y-tx@w)\n",
    "    # ***************************************************\n",
    "\n",
    "\n",
    "def least_squares_GD(y, tx, initial_w, max_iter, gamma):\n",
    "    \"\"\"least square gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iter):\n",
    "        gradient = compute_gradient(y,tx,w)\n",
    "        loss = compute_loss(y,tx,w)\n",
    "        w = w-gamma*gradient\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "\n",
    "    return np.array(ws)[-1], np.array(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## least square SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    # ***************************************************\n",
    "    e = y-tx@w\n",
    "    return -1/len(y)*tx.T@e\n",
    "    # ***************************************************\n",
    "\n",
    "\n",
    "def least_squares_SGD(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"Least square stochastic gradient descent algorithm.\"\"\"\n",
    "    # ***************************************************\n",
    "    # Define parameters to store w and loss\n",
    "    ws = []\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # compute random batch\n",
    "        a = batch_iter(y, tx, batch_size, num_batches=1, shuffle=True)\n",
    "        a = list(a)\n",
    "        tx2, y2 = a[0][1], a[0][0]\n",
    "        \n",
    "        # compute gradient & loss\n",
    "        grad = compute_stoch_gradient(y2,tx2,w)\n",
    "        loss= compute_loss(y2, tx2, w)\n",
    "        print(grad)\n",
    "        # update gradient\n",
    "        w = w-gamma*grad\n",
    "        \n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        #print(\"stoch Gradient Descent({bi}/{ti}): loss={l}\".format(\n",
    "              #bi=n_iter, ti=max_iters - 1, l=loss))\n",
    "\n",
    "    return np.array(losses), np.array(ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## least square\n",
    "computed by solving for w:  X<sup>T</sup>X * w = X<sup>T</sup>y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_square(y, tx):\n",
    "    w = np.linalg.solve(tx.T@tx,tx.T@y)\n",
    "    return w, compute_loss(y, tx, w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_std' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-f7f9a870b698>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mwls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mleast_square\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_std\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtX_std\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'y_std' is not defined"
     ]
    }
   ],
   "source": [
    "wls, loss = least_square(y_std, tX_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lamda):\n",
    "    w = np.linalg.solve(tx.T@tx+lamda*np.eye(tx.shape[1]),tx.T@y)\n",
    "    return w, compute_loss(y, tx, w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wls, loss = ridge_regression(y, tX_std,0)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "def update_weights(tx, y, w, gamma):\n",
    "    #probabilities array that the label is 1\n",
    "    probabilities = sigmoid(np.dot(tx, w))\n",
    "    gradient = np.dot(tx.T,  probabilities - y)\n",
    "    w -= gradient*gamma / len(tx)\n",
    "    return w\n",
    "\n",
    "def loss_function_LR(tx, y, w):\n",
    "    #probabilities array that the label is 1\n",
    "    probabilities = sigmoid(np.dot(tx, w))\n",
    "    #the error when label=1\n",
    "    error1 = -y*np.log(probabilities)\n",
    "    #the error when label=-1\n",
    "    error2 = (1-y)*np.log(1-probabilities)\n",
    "    #return average of sum of costs\n",
    "    return (error1-error2).mean()\n",
    "\n",
    "\n",
    "# logistic regression function\n",
    "def logistic_regression(y,tx, initial_w,  max_iter, gamma):\n",
    "    losses = []\n",
    "    ws = []\n",
    "    for iter_n in range(max_iter):\n",
    "        w = update_weights(tx, y, initial_w, gamma)\n",
    "        loss = loss_function_LR(tx, y, w)\n",
    "        losses.append(loss)\n",
    "        ws.append(w)\n",
    "    return np.array(losses)[-1], np.array(ws)[-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## regularized logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_LR_update_weights(y, tx, w, gamma, lambda_):\n",
    "    \"\"\"\n",
    "    Update weights function for  regularized logistic regression\n",
    "    \n",
    "    :param tx: features matrix\n",
    "    :param y: labels vector\n",
    "    :param w: weights\n",
    "    :param gamma: learning rate\n",
    "    :param lambda_: regulizer\n",
    "    \n",
    "    :return w: new updated weights\n",
    "    \"\"\" \n",
    "    # probabilities array that the label is 1\n",
    "    probabilities = sigmoid(np.dot(tx, w))\n",
    "    gradient = np.dot(tx.T,  probabilities - y) + lambda_ * w\n",
    "    w -= gradient*gamma / len(tx)\n",
    "    return w\n",
    "\n",
    "def reg_LR_loss_function(y, tx, w, lambda_):\n",
    "    \"\"\"\n",
    "    Computes logistic loss\n",
    "    \n",
    "    :param tx: features matrix\n",
    "    :param y: labels vector\n",
    "    :param w: weights\n",
    "    :param lambda_: regulizer\n",
    "    \n",
    "    :return w: logistic loss\n",
    "    \"\"\" \n",
    "    # probabilities array that the label is 1\n",
    "    probabilities = sigmoid(np.dot(tx, w))\n",
    "    # the error when label=1\n",
    "    error1 = -y*np.log(probabilities)\n",
    "    # the error when label=0\n",
    "    error2 = (1-y)*np.log(1-probabilities)\n",
    "    # return average of sum of costs\n",
    "    return (error1-error2).mean()+lambda_/2*np.dot(w.T,w)/ len(tx)\n",
    "\n",
    "\n",
    "# regularized logistic regression function\n",
    "def reg_logistic_regression(y,tx, initial_w,max_iter, gamma,lambda_):\n",
    "    \"\"\"\n",
    "    Regularized logistic regression function\n",
    "    \n",
    "    :param tx: features matrix\n",
    "    :param y: labels vector\n",
    "    :param initial_w: initial weights\n",
    "    :param max_iter: number of iterations\n",
    "    :param gamma: learning rate\n",
    "    :param lambda_: regulizer\n",
    "\n",
    "    :return ls: last loss  computed\n",
    "    :return ws: last weights computed\n",
    "    \"\"\" \n",
    "    losses = []\n",
    "    ws = []\n",
    "    for iter_n in range(max_iter):\n",
    "        w = reg_LR_update_weights(y, tx, initial_w, gamma,lambda_)\n",
    "        loss = reg_LR_loss_function(y, tx, w, lambda_)\n",
    "        losses.append(loss)\n",
    "        ws.append(w)\n",
    "    ls, wes  = np.array(losses), np.array(ws)\n",
    "    return wes[-1],ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_pred, y):\n",
    "    \"\"\"\n",
    "    compute the accuracy\n",
    "    \n",
    "    :param y_pred: predictions\n",
    "    :param y: real labels\n",
    "    \n",
    "    :return acc: accuracy\n",
    "    \"\"\"\n",
    "    # y_pred - y & count 0\n",
    "    arr = np.array(y_pred) - np.array(y)\n",
    "    acc = np.count_nonzero(arr==0) / len(y)\n",
    "    return acc\n",
    "\n",
    "\n",
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"\n",
    "    build k indices for k-fold.\n",
    "    \n",
    "    :param y: labels\n",
    "    :param k_fold: number of folds\n",
    "    :param seed: seed for randomization\n",
    "    \n",
    "    :return k_indices: indices \n",
    "    \"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval] for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "\n",
    "def build_poly(x, degree):\n",
    "    \"\"\"\n",
    "    polynomial basis functions for input data x, for j=0 up to j=degree.\n",
    "    \n",
    "    :param x: matrix \n",
    "    :param degree: degree of expansion\n",
    "    \"\"\"\n",
    "    poly = np.ones((len(x), 1))\n",
    "    for deg in range(1, degree+1):\n",
    "        poly = np.c_[poly, np.power(x, deg)]\n",
    "    return poly\n",
    "\n",
    "\n",
    "def cross_validation(y, x, k_indices, k, degree, logistic, regression_method, **kwargs):\n",
    "    \"\"\"\n",
    "    Computes cross validation on a given data set using a given regression method, and computes the\n",
    "    weights, the train loss, the test loss, and the train and loss accuracy\n",
    "    if the degree is not none, it will perform feature expansion on the data set\n",
    "    \n",
    "    :param y: labels vector\n",
    "    :param tx: features matrix\n",
    "    :param k_indices: k_fold already randomly computed indices\n",
    "    :param degree: degree of polynomial expansion\n",
    "    :param logistic: boolean; if true, the loss used is the logistic one\n",
    "    :param **kwargs: differents parameters such as the regulizer lambda or the learning rate gamma\n",
    "    \"\"\"\n",
    "    test_indice = k_indices[k]\n",
    "    train_indice = k_indices[~(np.arange(k_indices.shape[0]) == k)]\n",
    "    train_indice = train_indice.reshape(-1)\n",
    "    \n",
    "    y_test = y[test_indice]\n",
    "    y_train = y[train_indice]\n",
    "    x_test = x[test_indice]\n",
    "    x_train = x[train_indice]\n",
    "    \n",
    "    if degree != None:\n",
    "        x_train = build_poly(x_train, degree)\n",
    "        x_test = build_poly(x_test, degree)\n",
    "\n",
    "    \n",
    "    w_initial = np.zeros(x_train.shape[1])\n",
    "    kwargs = kwargs\n",
    "    kwargs['initial_w'] = w_initial\n",
    "\n",
    "    w, loss_train = regression_method(y = y_train, tx = x_train, **kwargs)\n",
    "    if logistic == True:\n",
    "        loss_test = reg_LR_loss_function(y_test, x_test, w ,kwargs['lambda_'])\n",
    "    else:\n",
    "        loss_test = compute_loss(y_test, x_test, w)\n",
    "    y_train_pred = predict_labels(w, x_train)\n",
    "    y_test_pred = predict_labels(w, x_test)\n",
    "    y_test = (y_test*2)-1\n",
    "    y_train = (y_train*2)-1\n",
    "    accuracy_train = compute_accuracy(y_train_pred, y_train)\n",
    "    accuracy_test = compute_accuracy(y_test_pred, y_test)\n",
    "    return w, loss_train, loss_test, accuracy_train, accuracy_test\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = \"train.csv\" \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "\n",
    "# standardize the data\n",
    "tX_std = standardize(tX)\n",
    "y_std = (y+1)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,y,tx,k_fold,degree,seed=0, **kwargs):\n",
    "    \"\"\"\n",
    "    regularized logistic regression function \n",
    "    \n",
    "    :param Model: model that we'll use\n",
    "    :param y: labels vector\n",
    "    :param tx: features matrix\n",
    "    :param k_fold: number of folds\n",
    "    :param degree: degree of polynomial expansion\n",
    "    :param seed: random seed for cross validation split\n",
    "    :param **kwargs: multiple possible parameters\n",
    "    \n",
    "    :return wf: final weights \n",
    "    \"\"\"    \n",
    "    weights = []\n",
    "    losses_train = []\n",
    "    losses_test = []\n",
    "    accuracies_train = []\n",
    "    accuracies_test = []\n",
    "    \n",
    "    logistic = False\n",
    "    if model is logistic_regression or model is reg_logistic_regression:\n",
    "        logistic = True\n",
    "    \n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    for k in range(k_fold):\n",
    "        w, loss_train, loss_test, accuracy_train, accuracy_test = cross_validation(y, tx, k_indices, k, degree, logistic, model,max_iter=200, **kwargs)\n",
    "        weights.append(w)\n",
    "        losses_train.append(loss_train)\n",
    "        losses_test.append(loss_test)\n",
    "        accuracies_train.append(accuracy_train)\n",
    "        accuracies_test.append(accuracy_test)\n",
    "    leg = [\"train loss \"+str(i) for i in range(k_fold)]\n",
    "    plt.legend(leg)\n",
    "    for ls in losses_train:\n",
    "        plt.plot(ls)\n",
    "    \n",
    "    print(\"<-\"+\"-\"*100+\"->\")\n",
    "    for i in range(k_fold):\n",
    "        print(f'loss_train: {losses_train[i][-1]}, loss_test: {losses_test[i]}, accuracy_train: {accuracies_train[i]}, accuracy_test: {accuracies_test[i]}')\n",
    "        print(\"\\n\")\n",
    "    print(\"<-\"+\"-\"*100+\"->\")\n",
    "\n",
    "    return w, np.amin(losses_train), np.amin(losses_test), np.max(accuracies_train), np.max(accuracies_test) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done about : 0.0\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "loss_train: 0.33843849809698817, loss_test: 0.33663047349495595, accuracy_train: 0.34303, accuracy_test: 0.34122\n",
      "\n",
      "\n",
      "loss_train: 0.33800540313766353, loss_test: 0.33839221659057706, accuracy_train: 0.342585, accuracy_test: 0.343\n",
      "\n",
      "\n",
      "loss_train: 0.3374116972953924, loss_test: 0.3408072924747678, accuracy_train: 0.341975, accuracy_test: 0.34544\n",
      "\n",
      "\n",
      "loss_train: 0.3380005368143699, loss_test: 0.33841201181414376, accuracy_train: 0.34258, accuracy_test: 0.34302\n",
      "\n",
      "\n",
      "loss_train: 0.3385747494951707, loss_test: 0.33607623024674105, accuracy_train: 0.34317, accuracy_test: 0.34066\n",
      "\n",
      "\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "loss_train: 0.33771342028674556, loss_test: 0.339579944059319, accuracy_train: 0.342285, accuracy_test: 0.3442\n",
      "\n",
      "\n",
      "loss_train: 0.3380978629095509, loss_test: 0.3380161088515091, accuracy_train: 0.34268, accuracy_test: 0.34262\n",
      "\n",
      "\n",
      "loss_train: 0.338929969162302, loss_test: 0.33463126819379907, accuracy_train: 0.343535, accuracy_test: 0.3392\n",
      "\n",
      "\n",
      "loss_train: 0.33754796022017036, loss_test: 0.34025300231390543, accuracy_train: 0.342115, accuracy_test: 0.34488\n",
      "\n",
      "\n",
      "loss_train: 0.3381416593977751, loss_test: 0.33783795355456275, accuracy_train: 0.342725, accuracy_test: 0.34244\n",
      "\n",
      "\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "loss_train: 0.3387937217519938, loss_test: 0.33518549521157215, accuracy_train: 0.343395, accuracy_test: 0.33976\n",
      "\n",
      "\n",
      "loss_train: 0.3381319268584935, loss_test: 0.3378775435649672, accuracy_train: 0.342715, accuracy_test: 0.34248\n",
      "\n",
      "\n",
      "loss_train: 0.33808326404502465, loss_test: 0.3380754940934209, accuracy_train: 0.342665, accuracy_test: 0.34268\n",
      "\n",
      "\n",
      "loss_train: 0.3376258240618577, loss_test: 0.33993626787419334, accuracy_train: 0.342195, accuracy_test: 0.34456\n",
      "\n",
      "\n",
      "loss_train: 0.33779614947426817, loss_test: 0.3392434183742454, accuracy_train: 0.34237, accuracy_test: 0.34386\n",
      "\n",
      "\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "loss_train: 0.33834604091995046, loss_test: 0.3370065706731198, accuracy_train: 0.342935, accuracy_test: 0.3416\n",
      "\n",
      "\n",
      "loss_train: 0.3383849703434951, loss_test: 0.3368482136171412, accuracy_train: 0.342975, accuracy_test: 0.34144\n",
      "\n",
      "\n",
      "loss_train: 0.3380054031376636, loss_test: 0.33839221659057706, accuracy_train: 0.342585, accuracy_test: 0.343\n",
      "\n",
      "\n",
      "loss_train: 0.3384336319473348, loss_test: 0.3366502680118157, accuracy_train: 0.343025, accuracy_test: 0.34124\n",
      "\n",
      "\n",
      "loss_train: 0.3372608329873279, loss_test: 0.3414209781287654, accuracy_train: 0.34182, accuracy_test: 0.34606\n",
      "\n",
      "\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "loss_train: 0.33885697966843226, loss_test: 0.33492817475054726, accuracy_train: 0.34346, accuracy_test: 0.3395\n",
      "\n",
      "\n",
      "loss_train: 0.33773775245925325, loss_test: 0.3394809656784345, accuracy_train: 0.34231, accuracy_test: 0.3441\n",
      "\n",
      "\n",
      "loss_train: 0.3376160911089612, loss_test: 0.33997585956799, accuracy_train: 0.342185, accuracy_test: 0.3446\n",
      "\n",
      "\n",
      "loss_train: 0.3382195194310816, loss_test: 0.3375212346147639, accuracy_train: 0.342805, accuracy_test: 0.34212\n",
      "\n",
      "\n",
      "loss_train: 0.3380005368143699, loss_test: 0.3384120118141437, accuracy_train: 0.34258, accuracy_test: 0.34302\n",
      "\n",
      "\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "loss_train: 0.3382925127586953, loss_test: 0.3372243124548755, accuracy_train: 0.34288, accuracy_test: 0.34182\n",
      "\n",
      "\n",
      "loss_train: 0.3385942138557604, loss_test: 0.33599705314804695, accuracy_train: 0.34319, accuracy_test: 0.34058\n",
      "\n",
      "\n",
      "loss_train: 0.3383411747332277, loss_test: 0.33702636534084945, accuracy_train: 0.34293, accuracy_test: 0.34162\n",
      "\n",
      "\n",
      "loss_train: 0.33725109974177897, loss_test: 0.3414605710136413, accuracy_train: 0.34181, accuracy_test: 0.3461\n",
      "\n",
      "\n",
      "loss_train: 0.3379518734741271, loss_test: 0.3386099644865404, accuracy_train: 0.34253, accuracy_test: 0.34322\n",
      "\n",
      "\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "loss_train: 0.3387839897355841, loss_test: 0.3352250830939146, accuracy_train: 0.343385, accuracy_test: 0.3398\n",
      "\n",
      "\n",
      "loss_train: 0.3380346010364545, loss_test: 0.33827344541592763, accuracy_train: 0.342615, accuracy_test: 0.34288\n",
      "\n",
      "\n",
      "loss_train: 0.33779128306708095, loss_test: 0.33926321393925485, accuracy_train: 0.342365, accuracy_test: 0.34388\n",
      "\n",
      "\n",
      "loss_train: 0.3379421407826663, loss_test: 0.33864955511630607, accuracy_train: 0.34252, accuracy_test: 0.34326\n",
      "\n",
      "\n",
      "loss_train: 0.3378788780979476, loss_test: 0.33890689498398463, accuracy_train: 0.342455, accuracy_test: 0.34352\n",
      "\n",
      "\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "loss_train: 0.33830224516921, loss_test: 0.33718472296854607, accuracy_train: 0.34289, accuracy_test: 0.34178\n",
      "\n",
      "\n",
      "loss_train: 0.33822925190012654, loss_test: 0.33748164489021854, accuracy_train: 0.342815, accuracy_test: 0.34208\n",
      "\n",
      "\n",
      "loss_train: 0.3383411747332277, loss_test: 0.33702636534084945, accuracy_train: 0.34293, accuracy_test: 0.34162\n",
      "\n",
      "\n",
      "loss_train: 0.3376501564099568, loss_test: 0.33983728877866093, accuracy_train: 0.34222, accuracy_test: 0.34446\n",
      "\n",
      "\n",
      "loss_train: 0.3379080763010969, loss_test: 0.3387881225706125, accuracy_train: 0.342485, accuracy_test: 0.3434\n",
      "\n",
      "\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "loss_train: 0.30454927848742, loss_test: 0.30184340587890435, accuracy_train: 0.34325, accuracy_test: 0.34034\n",
      "\n",
      "\n",
      "loss_train: 0.3039914652039809, loss_test: 0.3044382443213537, accuracy_train: 0.34253, accuracy_test: 0.34322\n",
      "\n",
      "\n",
      "loss_train: 0.3034682059535835, loss_test: 0.30687230244791286, accuracy_train: 0.341855, accuracy_test: 0.34592\n",
      "\n",
      "\n",
      "loss_train: 0.3046383421231402, loss_test: 0.30142909498863196, accuracy_train: 0.343365, accuracy_test: 0.33988\n",
      "\n",
      "\n",
      "loss_train: 0.30384420768364806, loss_test: 0.3051232498860945, accuracy_train: 0.34234, accuracy_test: 0.34398\n",
      "\n",
      "\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "loss_train: 0.30478934321455087, loss_test: 0.30072665751638605, accuracy_train: 0.34356, accuracy_test: 0.3391\n",
      "\n",
      "\n",
      "loss_train: 0.30383258107988703, loss_test: 0.30517733383515483, accuracy_train: 0.342325, accuracy_test: 0.34404\n",
      "\n",
      "\n",
      "loss_train: 0.3044989343284593, loss_test: 0.30207759895875524, accuracy_train: 0.343185, accuracy_test: 0.3406\n",
      "\n",
      "\n",
      "loss_train: 0.3039255897712314, loss_test: 0.30474468093726287, accuracy_train: 0.342445, accuracy_test: 0.34356\n",
      "\n",
      "\n",
      "loss_train: 0.30344494303972247, loss_test: 0.3069805141892978, accuracy_train: 0.341825, accuracy_test: 0.34604\n",
      "\n",
      "\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "loss_train: 0.3041851889672674, loss_test: 0.30353708441658944, accuracy_train: 0.34278, accuracy_test: 0.34222\n",
      "\n",
      "\n",
      "loss_train: 0.3036891741467152, loss_test: 0.30584442410370943, accuracy_train: 0.34214, accuracy_test: 0.34478\n",
      "\n",
      "\n",
      "loss_train: 0.3040069646160136, loss_test: 0.30436614470396334, accuracy_train: 0.34255, accuracy_test: 0.34314\n",
      "\n",
      "\n",
      "loss_train: 0.3041619442835864, loss_test: 0.30364521381275694, accuracy_train: 0.34275, accuracy_test: 0.34234\n",
      "\n",
      "\n",
      "loss_train: 0.30444858739391256, loss_test: 0.3023118045758509, accuracy_train: 0.34312, accuracy_test: 0.34086\n",
      "\n",
      "\n",
      "<------------------------------------------------------------------------------------------------------>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<------------------------------------------------------------------------------------------------------>\n",
      "loss_train: 0.30454927848742014, loss_test: 0.3018434058789044, accuracy_train: 0.34325, accuracy_test: 0.34034\n",
      "\n",
      "\n",
      "loss_train: 0.30418518896726754, loss_test: 0.3035370844165894, accuracy_train: 0.34278, accuracy_test: 0.34222\n",
      "\n",
      "\n",
      "loss_train: 0.3042316765608827, loss_test: 0.3033208336362216, accuracy_train: 0.34284, accuracy_test: 0.34198\n",
      "\n",
      "\n",
      "loss_train: 0.3032433063528273, loss_test: 0.3079184611521, accuracy_train: 0.341565, accuracy_test: 0.34708\n",
      "\n",
      "\n",
      "loss_train: 0.30428203545179955, loss_test: 0.30308657401253286, accuracy_train: 0.342905, accuracy_test: 0.34172\n",
      "\n",
      "\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "loss_train: 0.3043866181257371, loss_test: 0.3026000748539379, accuracy_train: 0.34304, accuracy_test: 0.34118\n",
      "\n",
      "\n",
      "loss_train: 0.30416969257717447, loss_test: 0.3036091703839615, accuracy_train: 0.34276, accuracy_test: 0.3423\n",
      "\n",
      "\n",
      "loss_train: 0.30381320307848036, loss_test: 0.30526747523395287, accuracy_train: 0.3423, accuracy_test: 0.34414\n",
      "\n",
      "\n",
      "loss_train: 0.3040650850707756, loss_test: 0.3040957817100959, accuracy_train: 0.342625, accuracy_test: 0.34284\n",
      "\n",
      "\n",
      "loss_train: 0.304057335890314, loss_test: 0.30413182914487474, accuracy_train: 0.342615, accuracy_test: 0.34288\n",
      "\n",
      "\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "loss_train: 0.3037008025734939, loss_test: 0.3057903319201275, accuracy_train: 0.342155, accuracy_test: 0.34472\n",
      "\n",
      "\n",
      "loss_train: 0.3035806350356085, loss_test: 0.3063493166800461, accuracy_train: 0.342, accuracy_test: 0.34534\n",
      "\n",
      "\n",
      "loss_train: 0.30420455908535315, loss_test: 0.303446978626534, accuracy_train: 0.342805, accuracy_test: 0.34212\n",
      "\n",
      "\n",
      "loss_train: 0.30409608113567893, loss_test: 0.30395159493837554, accuracy_train: 0.342665, accuracy_test: 0.34268\n",
      "\n",
      "\n",
      "loss_train: 0.304909351903518, loss_test: 0.3001683902726321, accuracy_train: 0.343715, accuracy_test: 0.33848\n",
      "\n",
      "\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "loss_train: 0.3045066797643435, loss_test: 0.3020415684381291, accuracy_train: 0.343195, accuracy_test: 0.34056\n",
      "\n",
      "\n",
      "loss_train: 0.3034255568265231, loss_test: 0.30707069268053605, accuracy_train: 0.3418, accuracy_test: 0.34614\n",
      "\n",
      "\n",
      "loss_train: 0.3048164433303505, loss_test: 0.30060059093949226, accuracy_train: 0.343595, accuracy_test: 0.33896\n",
      "\n",
      "\n",
      "loss_train: 0.30383258107988703, loss_test: 0.3051773338351549, accuracy_train: 0.342325, accuracy_test: 0.34404\n",
      "\n",
      "\n",
      "loss_train: 0.3039100889796174, loss_test: 0.304816786786183, accuracy_train: 0.342425, accuracy_test: 0.34364\n",
      "\n",
      "\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "loss_train: 0.3039720905694091, loss_test: 0.30452837051225107, accuracy_train: 0.342505, accuracy_test: 0.34332\n",
      "\n",
      "\n",
      "loss_train: 0.30461510896881605, loss_test: 0.3015371723070131, accuracy_train: 0.343335, accuracy_test: 0.34\n",
      "\n",
      "\n",
      "loss_train: 0.3040418373323077, loss_test: 0.30420392490465115, accuracy_train: 0.342595, accuracy_test: 0.34296\n",
      "\n",
      "\n",
      "loss_train: 0.30396821559322396, loss_test: 0.30454639597298516, accuracy_train: 0.3425, accuracy_test: 0.34334\n",
      "\n",
      "\n",
      "loss_train: 0.30389458792522617, loss_test: 0.30488889382206114, accuracy_train: 0.342405, accuracy_test: 0.34372\n",
      "\n",
      "\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "loss_train: 0.22728158700560366, loss_test: 0.22795740402658313, accuracy_train: 0.342315, accuracy_test: 0.34408\n",
      "\n",
      "\n",
      "loss_train: 0.22738478093721481, loss_test: 0.227419238663533, accuracy_train: 0.34263, accuracy_test: 0.34282\n",
      "\n",
      "\n",
      "loss_train: 0.22745673734512933, loss_test: 0.2270442808588356, accuracy_train: 0.34285, accuracy_test: 0.34194\n",
      "\n",
      "\n",
      "loss_train: 0.22737496133713467, loss_test: 0.22747042685941796, accuracy_train: 0.3426, accuracy_test: 0.34294\n",
      "\n",
      "\n",
      "loss_train: 0.2274877800534421, loss_test: 0.22688259703010527, accuracy_train: 0.342945, accuracy_test: 0.34156\n",
      "\n",
      "\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "loss_train: 0.22759384479240755, loss_test: 0.22633051621033798, accuracy_train: 0.34327, accuracy_test: 0.34026\n",
      "\n",
      "\n",
      "loss_train: 0.22745673734512933, loss_test: 0.2270442808588356, accuracy_train: 0.34285, accuracy_test: 0.34194\n",
      "\n",
      "\n",
      "loss_train: 0.22721596593547244, loss_test: 0.22829988717042518, accuracy_train: 0.342115, accuracy_test: 0.34488\n",
      "\n",
      "\n",
      "loss_train: 0.22717984087144258, loss_test: 0.228488514724583, accuracy_train: 0.342005, accuracy_test: 0.34532\n",
      "\n",
      "\n",
      "loss_train: 0.22753839064250173, loss_test: 0.22661909462747887, accuracy_train: 0.3431, accuracy_test: 0.34094\n",
      "\n",
      "\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "loss_train: 0.22733402731665392, loss_test: 0.22768385977316874, accuracy_train: 0.342475, accuracy_test: 0.34344\n",
      "\n",
      "\n",
      "loss_train: 0.2273241985869035, loss_test: 0.22773511937588417, accuracy_train: 0.342445, accuracy_test: 0.34356\n",
      "\n",
      "\n",
      "loss_train: 0.2274763453277075, loss_test: 0.22694214863235673, accuracy_train: 0.34291, accuracy_test: 0.3417\n",
      "\n",
      "\n",
      "loss_train: 0.22749757933264325, loss_test: 0.22683156777200408, accuracy_train: 0.342975, accuracy_test: 0.34144\n",
      "\n",
      "\n",
      "loss_train: 0.22735367947505605, loss_test: 0.22758138202976855, accuracy_train: 0.342535, accuracy_test: 0.3432\n",
      "\n",
      "\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "loss_train: 0.22726682911207685, loss_test: 0.22803440917882484, accuracy_train: 0.34227, accuracy_test: 0.34426\n",
      "\n",
      "\n",
      "loss_train: 0.22724058304006833, loss_test: 0.22817138400879255, accuracy_train: 0.34219, accuracy_test: 0.34458\n",
      "\n",
      "\n",
      "loss_train: 0.2274076864656072, loss_test: 0.22729985328687796, accuracy_train: 0.3427, accuracy_test: 0.34254\n",
      "\n",
      "\n",
      "loss_train: 0.22738805374456758, loss_test: 0.22740217900283274, accuracy_train: 0.34264, accuracy_test: 0.34278\n",
      "\n",
      "\n",
      "loss_train: 0.22768180240631805, loss_test: 0.22587309794778623, accuracy_train: 0.34354, accuracy_test: 0.33918\n",
      "\n",
      "\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "loss_train: 0.22738314445991212, loss_test: 0.22742776906974455, accuracy_train: 0.342625, accuracy_test: 0.34284\n",
      "\n",
      "\n",
      "loss_train: 0.22747471159912272, loss_test: 0.22695065753973773, accuracy_train: 0.342905, accuracy_test: 0.34172\n",
      "\n",
      "\n",
      "loss_train: 0.22724878628738784, loss_test: 0.2281285688169662, accuracy_train: 0.342215, accuracy_test: 0.34448\n",
      "\n",
      "\n",
      "loss_train: 0.22740114301060868, loss_test: 0.2273339557163399, accuracy_train: 0.34268, accuracy_test: 0.34262\n",
      "\n",
      "\n",
      "loss_train: 0.22747797900720798, loss_test: 0.22693364010888337, accuracy_train: 0.342915, accuracy_test: 0.34168\n",
      "\n",
      "\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "loss_train: 0.22754981231182628, loss_test: 0.2265596451446732, accuracy_train: 0.343135, accuracy_test: 0.3408\n",
      "\n",
      "\n",
      "loss_train: 0.22740932220664611, loss_test: 0.22729132863928164, accuracy_train: 0.342705, accuracy_test: 0.34252\n",
      "\n",
      "\n",
      "loss_train: 0.22736350290370763, loss_test: 0.22753016388908373, accuracy_train: 0.342565, accuracy_test: 0.34308\n",
      "\n",
      "\n",
      "loss_train: 0.2274436614300039, loss_test: 0.22711239972243688, accuracy_train: 0.34281, accuracy_test: 0.3421\n",
      "\n",
      "\n",
      "loss_train: 0.2272192488541805, loss_test: 0.22828274842474083, accuracy_train: 0.342125, accuracy_test: 0.34484\n",
      "\n",
      "\n",
      "<------------------------------------------------------------------------------------------------------>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<------------------------------------------------------------------------------------------------------>\n",
      "loss_train: 0.22720447417384057, loss_test: 0.22835988487341255, accuracy_train: 0.34208, accuracy_test: 0.34502\n",
      "\n",
      "\n",
      "loss_train: 0.2274649081968446, loss_test: 0.2270017190460848, accuracy_train: 0.342875, accuracy_test: 0.34184\n",
      "\n",
      "\n",
      "loss_train: 0.22757264843509067, loss_test: 0.2264408026134921, accuracy_train: 0.343205, accuracy_test: 0.34052\n",
      "\n",
      "\n",
      "loss_train: 0.2273667769872509, loss_test: 0.2275130942467836, accuracy_train: 0.342575, accuracy_test: 0.34304\n",
      "\n",
      "\n",
      "loss_train: 0.2273765980598587, loss_test: 0.22746189453366783, accuracy_train: 0.342605, accuracy_test: 0.34292\n",
      "\n",
      "\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "loss_train: 0.22735040460616487, loss_test: 0.22759845781459165, accuracy_train: 0.342525, accuracy_test: 0.34324\n",
      "\n",
      "\n",
      "loss_train: 0.2272947017954542, loss_test: 0.22788898110809133, accuracy_train: 0.342355, accuracy_test: 0.34392\n",
      "\n",
      "\n",
      "loss_train: 0.2275367587791184, loss_test: 0.22662758894636764, accuracy_train: 0.343095, accuracy_test: 0.34096\n",
      "\n",
      "\n",
      "loss_train: 0.22733075126974078, loss_test: 0.22770094477177644, accuracy_train: 0.342465, accuracy_test: 0.34348\n",
      "\n",
      "\n",
      "loss_train: 0.22747307782145365, loss_test: 0.22695916683102627, accuracy_train: 0.3429, accuracy_test: 0.34174\n",
      "\n",
      "\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "loss_train: 0.22524727110000012, loss_test: 0.22524412449999515, accuracy_train: 0.34267, accuracy_test: 0.34266\n",
      "\n",
      "\n",
      "loss_train: 0.22514647750000008, loss_test: 0.2256478045007681, accuracy_train: 0.34235, accuracy_test: 0.34394\n",
      "\n",
      "\n",
      "loss_train: 0.2253604191, loss_test: 0.22479218769912396, accuracy_train: 0.34303, accuracy_test: 0.34122\n",
      "\n",
      "\n",
      "loss_train: 0.22499015897500005, loss_test: 0.22627587782696082, accuracy_train: 0.341855, accuracy_test: 0.34592\n",
      "\n",
      "\n",
      "loss_train: 0.22548740077499999, loss_test: 0.22428654722314154, accuracy_train: 0.343435, accuracy_test: 0.3396\n",
      "\n",
      "\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "loss_train: 0.22539964597499998, loss_test: 0.22463581082382095, accuracy_train: 0.343155, accuracy_test: 0.34072\n",
      "\n",
      "\n",
      "loss_train: 0.2253133056000001, loss_test: 0.22498021119948727, accuracy_train: 0.34288, accuracy_test: 0.34182\n",
      "\n",
      "\n",
      "loss_train: 0.22502177797499998, loss_test: 0.22614863882672015, accuracy_train: 0.341955, accuracy_test: 0.34552\n",
      "\n",
      "\n",
      "loss_train: 0.22529287577499998, loss_test: 0.2250618138246445, accuracy_train: 0.342815, accuracy_test: 0.34208\n",
      "\n",
      "\n",
      "loss_train: 0.22520477377500003, loss_test: 0.22541420222532144, accuracy_train: 0.342535, accuracy_test: 0.3432\n",
      "\n",
      "\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "loss_train: 0.22511809240000005, loss_test: 0.22576167160098518, accuracy_train: 0.34226, accuracy_test: 0.3443\n",
      "\n",
      "\n",
      "loss_train: 0.22526457437499997, loss_test: 0.2251749276248622, accuracy_train: 0.342725, accuracy_test: 0.34244\n",
      "\n",
      "\n",
      "loss_train: 0.22507548437500005, loss_test: 0.22593274562631058, accuracy_train: 0.342125, accuracy_test: 0.34484\n",
      "\n",
      "\n",
      "loss_train: 0.22526771977500012, loss_test: 0.22516235222483796, accuracy_train: 0.342735, accuracy_test: 0.3424\n",
      "\n",
      "\n",
      "loss_train: 0.22550618497500002, loss_test: 0.22421188862299582, accuracy_train: 0.343495, accuracy_test: 0.33936\n",
      "\n",
      "\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "loss_train: 0.22521579510000006, loss_test: 0.2253700765002369, accuracy_train: 0.34257, accuracy_test: 0.34306\n",
      "\n",
      "\n",
      "loss_train: 0.225396508975, loss_test: 0.22464831062384524, accuracy_train: 0.343145, accuracy_test: 0.34076\n",
      "\n",
      "\n",
      "loss_train: 0.2253541399, loss_test: 0.22481723409917237, accuracy_train: 0.34301, accuracy_test: 0.3413\n",
      "\n",
      "\n",
      "loss_train: 0.225340008775, loss_test: 0.2248736148242814, accuracy_train: 0.342965, accuracy_test: 0.34148\n",
      "\n",
      "\n",
      "loss_train: 0.22492527750000005, loss_test: 0.22653728050245375, accuracy_train: 0.34165, accuracy_test: 0.34674\n",
      "\n",
      "\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "loss_train: 0.2252220918999999, loss_test: 0.2253448717001885, accuracy_train: 0.34259, accuracy_test: 0.34298\n",
      "\n",
      "\n",
      "loss_train: 0.22504231959999993, loss_test: 0.22606603000156358, accuracy_train: 0.34202, accuracy_test: 0.34526\n",
      "\n",
      "\n",
      "loss_train: 0.22528658797499992, loss_test: 0.22508693762469287, accuracy_train: 0.342795, accuracy_test: 0.34216\n",
      "\n",
      "\n",
      "loss_train: 0.2254059193749999, loss_test: 0.22461081662377247, accuracy_train: 0.343175, accuracy_test: 0.34064\n",
      "\n",
      "\n",
      "loss_train: 0.22527558239999992, loss_test: 0.2251309215997776, accuracy_train: 0.34276, accuracy_test: 0.3423\n",
      "\n",
      "\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "loss_train: 0.22525041760000003, loss_test: 0.22523153919997094, accuracy_train: 0.34268, accuracy_test: 0.34262\n",
      "\n",
      "\n",
      "loss_train: 0.22506127360000006, loss_test: 0.225989843201419, accuracy_train: 0.34208, accuracy_test: 0.34502\n",
      "\n",
      "\n",
      "loss_train: 0.22535100000000002, loss_test: 0.2248297599991966, accuracy_train: 0.343, accuracy_test: 0.34134\n",
      "\n",
      "\n",
      "loss_train: 0.225255136975, loss_test: 0.2252126646249346, accuracy_train: 0.342695, accuracy_test: 0.34256\n",
      "\n",
      "\n",
      "loss_train: 0.2253148767750001, loss_test: 0.22497393722447503, accuracy_train: 0.342885, accuracy_test: 0.3418\n",
      "\n",
      "\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "loss_train: 0.22522209189999998, loss_test: 0.2253448717001885, accuracy_train: 0.34259, accuracy_test: 0.34298\n",
      "\n",
      "\n",
      "loss_train: 0.22524097750000005, loss_test: 0.22526930050004346, accuracy_train: 0.34265, accuracy_test: 0.34274\n",
      "\n",
      "\n",
      "loss_train: 0.22522366597499996, loss_test: 0.22533857162517645, accuracy_train: 0.342595, accuracy_test: 0.34296\n",
      "\n",
      "\n",
      "loss_train: 0.22541062390000002, loss_test: 0.22459207569873613, accuracy_train: 0.34319, accuracy_test: 0.34058\n",
      "\n",
      "\n",
      "loss_train: 0.22513544077499995, loss_test: 0.22569206882585255, accuracy_train: 0.342315, accuracy_test: 0.34408\n",
      "\n",
      "\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "loss_train: 0.225321160975, loss_test: 0.22494884582442665, accuracy_train: 0.342905, accuracy_test: 0.34172\n",
      "\n",
      "\n",
      "loss_train: 0.22523310937499996, loss_test: 0.22530078062510403, accuracy_train: 0.342625, accuracy_test: 0.34284\n",
      "\n",
      "\n",
      "loss_train: 0.22515278309999995, loss_test: 0.2256225205007198, accuracy_train: 0.34237, accuracy_test: 0.34386\n",
      "\n",
      "\n",
      "loss_train: 0.225299162775, loss_test: 0.22503669722459602, accuracy_train: 0.342835, accuracy_test: 0.342\n",
      "\n",
      "\n",
      "loss_train: 0.22522681397499994, loss_test: 0.22532597282515235, accuracy_train: 0.342605, accuracy_test: 0.34292\n",
      "\n",
      "\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "loss_train: 0.3383752379993151, loss_test: 0.3368878028334927, accuracy_train: 0.342965, accuracy_test: 0.34148\n",
      "\n",
      "\n",
      "loss_train: 0.33705643319194756, loss_test: 0.3422524353812065, accuracy_train: 0.34161, accuracy_test: 0.3469\n",
      "\n",
      "\n",
      "loss_train: 0.3376598893355395, loss_test: 0.3397976971960318, accuracy_train: 0.34223, accuracy_test: 0.34442\n",
      "\n",
      "\n",
      "loss_train: 0.3387401955651657, loss_test: 0.3354032289575122, accuracy_train: 0.34334, accuracy_test: 0.33998\n",
      "\n",
      "\n",
      "loss_train: 0.33859907994103033, loss_test: 0.3359772588932249, accuracy_train: 0.343195, accuracy_test: 0.34056\n",
      "\n",
      "\n",
      "<------------------------------------------------------------------------------------------------------>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<------------------------------------------------------------------------------------------------------>\n",
      "loss_train: 0.33792267537633286, loss_test: 0.3387287364711238, accuracy_train: 0.3425, accuracy_test: 0.34334\n",
      "\n",
      "\n",
      "loss_train: 0.33780101587950456, loss_test: 0.3392236228171762, accuracy_train: 0.342375, accuracy_test: 0.34384\n",
      "\n",
      "\n",
      "loss_train: 0.33852122234259036, loss_test: 0.3362939679232431, accuracy_train: 0.343115, accuracy_test: 0.34088\n",
      "\n",
      "\n",
      "loss_train: 0.3377474853145993, loss_test: 0.33944137438166444, accuracy_train: 0.34232, accuracy_test: 0.34406\n",
      "\n",
      "\n",
      "loss_train: 0.33843849809698817, loss_test: 0.336630473494956, accuracy_train: 0.34303, accuracy_test: 0.34122\n",
      "\n",
      "\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "loss_train: 0.3379470071293723, loss_test: 0.338629759797453, accuracy_train: 0.342525, accuracy_test: 0.34324\n",
      "\n",
      "\n",
      "loss_train: 0.3377231531616017, loss_test: 0.33954035268314364, accuracy_train: 0.342295, accuracy_test: 0.34416\n",
      "\n",
      "\n",
      "loss_train: 0.3377182867251493, loss_test: 0.3395601483672611, accuracy_train: 0.34229, accuracy_test: 0.34418\n",
      "\n",
      "\n",
      "loss_train: 0.3392073252355247, loss_test: 0.33350303957212224, accuracy_train: 0.34382, accuracy_test: 0.33806\n",
      "\n",
      "\n",
      "loss_train: 0.33783508066153045, loss_test: 0.3390850541400283, accuracy_train: 0.34241, accuracy_test: 0.3437\n",
      "\n",
      "\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "loss_train: 0.3382438505890615, loss_test: 0.3374222603629545, accuracy_train: 0.34283, accuracy_test: 0.34202\n",
      "\n",
      "\n",
      "loss_train: 0.3385893477685395, loss_test: 0.3360168474108098, accuracy_train: 0.343185, accuracy_test: 0.3406\n",
      "\n",
      "\n",
      "loss_train: 0.3377523517393457, loss_test: 0.33942157874518997, accuracy_train: 0.342325, accuracy_test: 0.34404\n",
      "\n",
      "\n",
      "loss_train: 0.3383460409199504, loss_test: 0.3370065706731198, accuracy_train: 0.342935, accuracy_test: 0.3416\n",
      "\n",
      "\n",
      "loss_train: 0.3374992950654838, loss_test: 0.3404509623709945, accuracy_train: 0.342065, accuracy_test: 0.34508\n",
      "\n",
      "\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "loss_train: 0.3379129426614599, loss_test: 0.33876832719617583, accuracy_train: 0.34249, accuracy_test: 0.34338\n",
      "\n",
      "\n",
      "loss_train: 0.3377426188879017, loss_test: 0.3394611700260791, accuracy_train: 0.342315, accuracy_test: 0.34408\n",
      "\n",
      "\n",
      "loss_train: 0.33809786290955096, loss_test: 0.3380161088515093, accuracy_train: 0.34268, accuracy_test: 0.34262\n",
      "\n",
      "\n",
      "loss_train: 0.3382146531936325, loss_test: 0.3375410294889473, accuracy_train: 0.3428, accuracy_test: 0.34214\n",
      "\n",
      "\n",
      "loss_train: 0.3384628288159898, loss_test: 0.3365315010297658, accuracy_train: 0.343055, accuracy_test: 0.34112\n",
      "\n",
      "\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "loss_train: 0.33714403352567274, loss_test: 0.34189609484357736, accuracy_train: 0.3417, accuracy_test: 0.34654\n",
      "\n",
      "\n",
      "loss_train: 0.3380346010364544, loss_test: 0.3382734454159276, accuracy_train: 0.342615, accuracy_test: 0.34288\n",
      "\n",
      "\n",
      "loss_train: 0.33921705691245785, loss_test: 0.33346345307143166, accuracy_train: 0.34383, accuracy_test: 0.33802\n",
      "\n",
      "\n",
      "loss_train: 0.3379032099387828, loss_test: 0.33880791795298987, accuracy_train: 0.34248, accuracy_test: 0.34342\n",
      "\n",
      "\n",
      "loss_train: 0.3381319268584934, loss_test: 0.3378775435649672, accuracy_train: 0.342715, accuracy_test: 0.34248\n",
      "\n",
      "\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "loss_train: 0.33772801959610327, loss_test: 0.33952055700696665, accuracy_train: 0.3423, accuracy_test: 0.34414\n",
      "\n",
      "\n",
      "loss_train: 0.33761122462958615, loss_test: 0.339995655426799, accuracy_train: 0.34218, accuracy_test: 0.34462\n",
      "\n",
      "\n",
      "loss_train: 0.3385893477685396, loss_test: 0.3360168474108098, accuracy_train: 0.343185, accuracy_test: 0.3406\n",
      "\n",
      "\n",
      "loss_train: 0.33863314248329107, loss_test: 0.33583869933180455, accuracy_train: 0.34323, accuracy_test: 0.34042\n",
      "\n",
      "\n",
      "loss_train: 0.3378691453479566, loss_test: 0.3389464858519662, accuracy_train: 0.342445, accuracy_test: 0.34356\n",
      "\n",
      "\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "loss_train: 0.3375041615897321, loss_test: 0.34043116632955317, accuracy_train: 0.34207, accuracy_test: 0.34506\n",
      "\n",
      "\n",
      "loss_train: 0.33860394602434923, loss_test: 0.3359574646463433, accuracy_train: 0.3432, accuracy_test: 0.34054\n",
      "\n",
      "\n",
      "loss_train: 0.3375479602201704, loss_test: 0.3402530023139054, accuracy_train: 0.342115, accuracy_test: 0.34488\n",
      "\n",
      "\n",
      "loss_train: 0.3378642789700344, loss_test: 0.33896628129786766, accuracy_train: 0.34244, accuracy_test: 0.34358\n",
      "\n",
      "\n",
      "loss_train: 0.33891050534019246, loss_test: 0.33471044310090686, accuracy_train: 0.343515, accuracy_test: 0.33928\n",
      "\n",
      "\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "loss_train: 0.3040302132413552, loss_test: 0.3042579975034247, accuracy_train: 0.34258, accuracy_test: 0.34302\n",
      "\n",
      "\n",
      "loss_train: 0.3046886785958631, loss_test: 0.301194936627305, accuracy_train: 0.34343, accuracy_test: 0.33962\n",
      "\n",
      "\n",
      "loss_train: 0.3041735666993331, loss_test: 0.3035911487808411, accuracy_train: 0.342765, accuracy_test: 0.34228\n",
      "\n",
      "\n",
      "loss_train: 0.3037124308524606, loss_test: 0.30573624040420944, accuracy_train: 0.34217, accuracy_test: 0.34466\n",
      "\n",
      "\n",
      "loss_train: 0.30388683729948895, loss_test: 0.30492494778510953, accuracy_train: 0.342395, accuracy_test: 0.34376\n",
      "\n",
      "\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "loss_train: 0.30408833221799464, loss_test: 0.3039876411861963, accuracy_train: 0.342655, accuracy_test: 0.34272\n",
      "\n",
      "\n",
      "loss_train: 0.3036194004819835, loss_test: 0.3061689912261437, accuracy_train: 0.34205, accuracy_test: 0.34514\n",
      "\n",
      "\n",
      "loss_train: 0.3041774408050681, loss_test: 0.30357312725190577, accuracy_train: 0.34277, accuracy_test: 0.34226\n",
      "\n",
      "\n",
      "loss_train: 0.30421230701762225, loss_test: 0.3034109368298061, accuracy_train: 0.342815, accuracy_test: 0.34208\n",
      "\n",
      "\n",
      "loss_train: 0.3043943645141891, loss_test: 0.3025640400305886, accuracy_train: 0.34305, accuracy_test: 0.34114\n",
      "\n",
      "\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "loss_train: 0.30403408795476294, loss_test: 0.3042399732296485, accuracy_train: 0.342585, accuracy_test: 0.343\n",
      "\n",
      "\n",
      "loss_train: 0.3046577026334284, loss_test: 0.30133903259673167, accuracy_train: 0.34339, accuracy_test: 0.33978\n",
      "\n",
      "\n",
      "loss_train: 0.30347596012681516, loss_test: 0.3068362324609303, accuracy_train: 0.341865, accuracy_test: 0.34588\n",
      "\n",
      "\n",
      "loss_train: 0.3039837153994229, loss_test: 0.3044742945751579, accuracy_train: 0.34252, accuracy_test: 0.34326\n",
      "\n",
      "\n",
      "loss_train: 0.3043401384154429, loss_test: 0.302816290025563, accuracy_train: 0.34298, accuracy_test: 0.34142\n",
      "\n",
      "\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "loss_train: 0.3039139642021562, loss_test: 0.3047987602126756, accuracy_train: 0.34243, accuracy_test: 0.34362\n",
      "\n",
      "\n",
      "loss_train: 0.30390233848526893, loss_test: 0.3048528401557523, accuracy_train: 0.342415, accuracy_test: 0.34368\n",
      "\n",
      "\n",
      "loss_train: 0.30425104569355294, loss_test: 0.30323073229725905, accuracy_train: 0.342865, accuracy_test: 0.34188\n",
      "\n",
      "\n",
      "loss_train: 0.30465770263342845, loss_test: 0.30133903259673167, accuracy_train: 0.34339, accuracy_test: 0.33978\n",
      "\n",
      "\n",
      "loss_train: 0.3037666941998988, loss_test: 0.3054838221579259, accuracy_train: 0.34224, accuracy_test: 0.34438\n",
      "\n",
      "\n",
      "<------------------------------------------------------------------------------------------------------>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<------------------------------------------------------------------------------------------------------>\n",
      "loss_train: 0.30382870551245283, loss_test: 0.30519536196654473, accuracy_train: 0.34232, accuracy_test: 0.34406\n",
      "\n",
      "\n",
      "loss_train: 0.30363878258928656, loss_test: 0.3060788312811255, accuracy_train: 0.342075, accuracy_test: 0.34504\n",
      "\n",
      "\n",
      "loss_train: 0.3043323915671306, loss_test: 0.302852326926089, accuracy_train: 0.34297, accuracy_test: 0.34146\n",
      "\n",
      "\n",
      "loss_train: 0.3041696925771744, loss_test: 0.30360917038396157, accuracy_train: 0.34276, accuracy_test: 0.3423\n",
      "\n",
      "\n",
      "loss_train: 0.3045221704390288, loss_test: 0.3019695082870954, accuracy_train: 0.343215, accuracy_test: 0.34048\n",
      "\n",
      "\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "loss_train: 0.30394496539121824, loss_test: 0.30465455029527244, accuracy_train: 0.34247, accuracy_test: 0.34346\n",
      "\n",
      "\n",
      "loss_train: 0.303743438873734, loss_test: 0.305591999625896, accuracy_train: 0.34221, accuracy_test: 0.3445\n",
      "\n",
      "\n",
      "loss_train: 0.3045492784874201, loss_test: 0.30184340587890435, accuracy_train: 0.34325, accuracy_test: 0.34034\n",
      "\n",
      "\n",
      "loss_train: 0.3044137301979068, loss_test: 0.3024739542704511, accuracy_train: 0.343075, accuracy_test: 0.34104\n",
      "\n",
      "\n",
      "loss_train: 0.30384033216548456, loss_test: 0.30514127779492983, accuracy_train: 0.342335, accuracy_test: 0.344\n",
      "\n",
      "\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "loss_train: 0.30459187522324294, loss_test: 0.3016452522960501, accuracy_train: 0.343305, accuracy_test: 0.34012\n",
      "\n",
      "\n",
      "loss_train: 0.3036310297956362, loss_test: 0.30611489503657824, accuracy_train: 0.342065, accuracy_test: 0.34508\n",
      "\n",
      "\n",
      "loss_train: 0.30376281835326363, loss_test: 0.3055018515504587, accuracy_train: 0.342235, accuracy_test: 0.3444\n",
      "\n",
      "\n",
      "loss_train: 0.3041038299876689, loss_test: 0.3039155489872942, accuracy_train: 0.342675, accuracy_test: 0.34264\n",
      "\n",
      "\n",
      "loss_train: 0.30440211083694685, loss_test: 0.30252800550397896, accuracy_train: 0.34306, accuracy_test: 0.3411\n",
      "\n",
      "\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "loss_train: 0.30426266697607224, loss_test: 0.30317667238410007, accuracy_train: 0.34288, accuracy_test: 0.34182\n",
      "\n",
      "\n",
      "loss_train: 0.30419293706377243, loss_test: 0.3035010418780125, accuracy_train: 0.34279, accuracy_test: 0.34218\n",
      "\n",
      "\n",
      "loss_train: 0.30441760328537937, loss_test: 0.30245593734097825, accuracy_train: 0.34308, accuracy_test: 0.34102\n",
      "\n",
      "\n",
      "loss_train: 0.3040728341855431, loss_test: 0.30405973457205643, accuracy_train: 0.342635, accuracy_test: 0.3428\n",
      "\n",
      "\n",
      "loss_train: 0.3035457447296545, loss_test: 0.3065116159313656, accuracy_train: 0.341955, accuracy_test: 0.34552\n",
      "\n",
      "\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "loss_train: 0.2273700508744572, loss_test: 0.2274960261401143, accuracy_train: 0.342585, accuracy_test: 0.343\n",
      "\n",
      "\n",
      "loss_train: 0.22752206979987702, loss_test: 0.22670405509221225, accuracy_train: 0.34305, accuracy_test: 0.34114\n",
      "\n",
      "\n",
      "loss_train: 0.22749267991392186, loss_test: 0.22685708067347007, accuracy_train: 0.34296, accuracy_test: 0.3415\n",
      "\n",
      "\n",
      "loss_train: 0.2274403919603801, loss_test: 0.22712943327741406, accuracy_train: 0.3428, accuracy_test: 0.34214\n",
      "\n",
      "\n",
      "loss_train: 0.22716012627787593, loss_test: 0.22859148079856526, accuracy_train: 0.341945, accuracy_test: 0.34556\n",
      "\n",
      "\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "loss_train: 0.2276606405061438, loss_test: 0.22598311484774178, accuracy_train: 0.343475, accuracy_test: 0.33944\n",
      "\n",
      "\n",
      "loss_train: 0.22704004354979607, loss_test: 0.22921904882223237, accuracy_train: 0.34158, accuracy_test: 0.34702\n",
      "\n",
      "\n",
      "loss_train: 0.22736513997002145, loss_test: 0.22752162887597976, accuracy_train: 0.34257, accuracy_test: 0.34306\n",
      "\n",
      "\n",
      "loss_train: 0.22741259354147142, loss_test: 0.22727428049581228, accuracy_train: 0.342715, accuracy_test: 0.34248\n",
      "\n",
      "\n",
      "loss_train: 0.22750574404882742, loss_test: 0.22678905394771445, accuracy_train: 0.343, accuracy_test: 0.34134\n",
      "\n",
      "\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "loss_train: 0.22737659805985863, loss_test: 0.2274618945336679, accuracy_train: 0.342605, accuracy_test: 0.34292\n",
      "\n",
      "\n",
      "loss_train: 0.22730125801235743, loss_test: 0.22785477886262995, accuracy_train: 0.342375, accuracy_test: 0.34384\n",
      "\n",
      "\n",
      "loss_train: 0.22734221657496254, loss_test: 0.22764115399503404, accuracy_train: 0.3425, accuracy_test: 0.34334\n",
      "\n",
      "\n",
      "loss_train: 0.2274420267197342, loss_test: 0.2271209163079716, accuracy_train: 0.342805, accuracy_test: 0.34212\n",
      "\n",
      "\n",
      "loss_train: 0.2275237021050186, loss_test: 0.22669555731815427, accuracy_train: 0.343055, accuracy_test: 0.34112\n",
      "\n",
      "\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "loss_train: 0.22702027889029552, loss_test: 0.22932240648205862, accuracy_train: 0.34152, accuracy_test: 0.34726\n",
      "\n",
      "\n",
      "loss_train: 0.22761177369255492, loss_test: 0.2262372476219457, accuracy_train: 0.343325, accuracy_test: 0.34004\n",
      "\n",
      "\n",
      "loss_train: 0.22737496133713458, loss_test: 0.22747042685941796, accuracy_train: 0.3426, accuracy_test: 0.34294\n",
      "\n",
      "\n",
      "loss_train: 0.2275285987259379, loss_test: 0.22667006629942654, accuracy_train: 0.34307, accuracy_test: 0.34106\n",
      "\n",
      "\n",
      "loss_train: 0.2274485652663078, loss_test: 0.22708685226927872, accuracy_train: 0.342825, accuracy_test: 0.34204\n",
      "\n",
      "\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "loss_train: 0.2274158646799597, loss_test: 0.22725723388797364, accuracy_train: 0.342725, accuracy_test: 0.34244\n",
      "\n",
      "\n",
      "loss_train: 0.22757917127470464, loss_test: 0.2264068614252602, accuracy_train: 0.343225, accuracy_test: 0.34044\n",
      "\n",
      "\n",
      "loss_train: 0.2272438644862489, loss_test: 0.2281542567803389, accuracy_train: 0.3422, accuracy_test: 0.34454\n",
      "\n",
      "\n",
      "loss_train: 0.22718476841543875, loss_test: 0.22846278184401053, accuracy_train: 0.34202, accuracy_test: 0.34526\n",
      "\n",
      "\n",
      "loss_train: 0.2275612315760226, loss_test: 0.2265002144733442, accuracy_train: 0.34317, accuracy_test: 0.34066\n",
      "\n",
      "\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "loss_train: 0.22735531683587532, loss_test: 0.22757284471321848, accuracy_train: 0.34254, accuracy_test: 0.34318\n",
      "\n",
      "\n",
      "loss_train: 0.2273078134439126, loss_test: 0.2278205827596915, accuracy_train: 0.342395, accuracy_test: 0.34376\n",
      "\n",
      "\n",
      "loss_train: 0.22750411120375913, loss_test: 0.22679755594475712, accuracy_train: 0.342995, accuracy_test: 0.34136\n",
      "\n",
      "\n",
      "loss_train: 0.22726682911207685, loss_test: 0.22803440917882478, accuracy_train: 0.34227, accuracy_test: 0.34426\n",
      "\n",
      "\n",
      "loss_train: 0.22755144378253575, loss_test: 0.22655115389704603, accuracy_train: 0.34314, accuracy_test: 0.34078\n",
      "\n",
      "\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "<------------------------------------------------------------------------------------------------------>\n",
      "loss_train: 0.2272979800020742, loss_test: 0.22787187921754526, accuracy_train: 0.342365, accuracy_test: 0.34388\n",
      "\n",
      "\n",
      "loss_train: 0.22713876082532153, loss_test: 0.22870308976371212, accuracy_train: 0.34188, accuracy_test: 0.34582\n",
      "\n",
      "\n",
      "loss_train: 0.2277013291053801, loss_test: 0.22577160147244227, accuracy_train: 0.3436, accuracy_test: 0.33894\n",
      "\n",
      "\n",
      "loss_train: 0.22720775777972801, loss_test: 0.22834274075302055, accuracy_train: 0.34209, accuracy_test: 0.34498\n",
      "\n",
      "\n",
      "loss_train: 0.22763784149057156, loss_test: 0.226101667144709, accuracy_train: 0.343405, accuracy_test: 0.33972\n",
      "\n",
      "\n",
      "<------------------------------------------------------------------------------------------------------>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<------------------------------------------------------------------------------------------------------>\n",
      "loss_train: 0.22747797900720806, loss_test: 0.22693364010888342, accuracy_train: 0.342915, accuracy_test: 0.34168\n",
      "\n",
      "\n",
      "loss_train: 0.2273422165749626, loss_test: 0.22764115399503407, accuracy_train: 0.3425, accuracy_test: 0.34334\n",
      "\n",
      "\n",
      "loss_train: 0.2272832265261298, loss_test: 0.22794884981809474, accuracy_train: 0.34232, accuracy_test: 0.34406\n",
      "\n",
      "\n",
      "loss_train: 0.2275742792186205, loss_test: 0.2264323167405726, accuracy_train: 0.34321, accuracy_test: 0.3405\n",
      "\n",
      "\n",
      "loss_train: 0.22730781344391257, loss_test: 0.2278205827596915, accuracy_train: 0.342395, accuracy_test: 0.34376\n",
      "\n",
      "\n",
      "<------------------------------------------------------------------------------------------------------>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-1eaf1ca11c16>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mdegree\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdegrees\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m                 \u001b[0mparameters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'gamma'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'gamma'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'lambda_'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'gamma'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'gamma'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'lambda_'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m                 \u001b[0mwTemp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlossTr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlossTe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccTr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccTe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_std\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtX_std\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlossTr\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mbestParametersLossTrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m                     \u001b[0mbestParametersLossTrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlossTr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwTemp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-47-0dfb4a174269>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, y, tx, k_fold, degree, seed, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mk_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_k_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk_fold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogistic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[0mweights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mlosses_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-3f1eadc12807>\u001b[0m in \u001b[0;36mcross_validation\u001b[1;34m(y, x, k_indices, k, degree, logistic, regression_method, **kwargs)\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'initial_w'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw_initial\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m     \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregression_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlogistic\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[0mloss_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreg_LR_loss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lambda_'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-d7e9a4bced7f>\u001b[0m in \u001b[0;36mleast_squares_GD\u001b[1;34m(y, tx, initial_w, max_iter, gamma)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mgradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mws\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Workspace\\Machine Learning\\ML_course\\projects\\project1VersionFinale\\costs.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[1;34m(y, tx, w)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubtract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m@\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#wf = train(reg_logistic_regression,y_std,tX_std,5,0,5,gamma=0.1757, lambda_ = 0)\n",
    "#w, lossTr, lossTe, accTr, accTe = train(names[0],y_std,tX_std,5,degrees[0],None,gamma = gammas[3], lambda_ = 0)\n",
    "\n",
    "names = [least_squares_GD, least_squares_SGD,least_square, ridge_regression, logistic_regression,reg_logistic_regression]\n",
    "initial_w  = np.zeros(tX_std.shape[1])\n",
    "lambdas = np.logspace(-5, 0, 15)\n",
    "gammas = np.array([0.0001,0.001,0.01,0.1])\n",
    "degrees = np.array([0,1,2,3,4,5,6,7])\n",
    "parameters = [{'gamma' : gammas[0],'lambda_' : lambdas[0]}]\n",
    "#wf = train(reg_logistic_regression,y_std,tX_std,5,0,None,**(parameters[0]))\n",
    "bestParametersLossTrain = [names[0], lambdas[0], gammas[0],degrees[0], 1, initial_w ]\n",
    "bestParametersLossTest = [names[0], lambdas[0], gammas[0],degrees[0], 1, initial_w ]\n",
    "bestParametersAccuracyTrain = [names[0], lambdas[0], gammas[0],degrees[0], 0, initial_w ]\n",
    "bestParametersAccuracyTest = [names[0], lambdas[0], gammas[0],degrees[0], 0, initial_w ]\n",
    "\n",
    "for index, model_name in enumerate(names):\n",
    "    print(\"Done about : \" + str(index/len(names)))\n",
    "    for lambda_ in lambdas:\n",
    "        for gamma in gammas:\n",
    "            for degree in degrees:\n",
    "                parameters = [{'gamma' : gamma}, {'gamma' : gamma}, {}, {'lambda_' : lambda_}, {'gamma' : gamma}, {'gamma' : gamma,'lambda_' : lambda_}]\n",
    "                wTemp, lossTr, lossTe, accTr, accTe = train(model_name,y_std,tX_std,5,0,None,**parameters[index])\n",
    "                if lossTr < bestParametersLossTrain[4]:\n",
    "                    bestParametersLossTrain = [model_name,lambda_, gamma, None, lossTr, wTemp]             \n",
    "                if lossTe < bestParametersLossTest[4]:\n",
    "                    bestParametersLossTrain = [model_name,lambda_, gamma, None, lossTe, wTemp]\n",
    "                if accTr > bestParametersAccuracyTrain[4]:    \n",
    "                    bestParametersLossTrain = [model_name,lambda_, gamma, None, accTr, wTemp]                    \n",
    "                if accTe > bestParametersAccuracyTest[4]:\n",
    "                    bestParametersLossTrain = [model_name,lambda_, gamma, None, accTe, wTemp]                    \n",
    "                           \n",
    "                \n",
    "\n",
    "#The parameters are : Lambda, degrees, gamma\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test data\n",
    "DATA_TEST_PATH = '../data/test.csv' \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "\n",
    "# standardize wrt the tx train mean and std\n",
    "def standardize_test(tx, tx_test):\n",
    "    centered_data = tx_test - np.mean(tx, axis=0)\n",
    "    std_data = centered_data / np.std(tx, axis=0)\n",
    "    return std_data\n",
    "tX_test_std = standardize_test(tX, tX_test)\n",
    "OUTPUT_PATH = 'outLogRegD23v2.csv' \n",
    "y_pred = predict_labels(wk, tX_test_std)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xsdfqfdsf = np.array([1,2,3])\n",
    "np.max(xsdfqfdsf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
